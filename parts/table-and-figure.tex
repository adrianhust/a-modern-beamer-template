\section{Model Architecture}


\subsection{Model}
\begin{frame}{Model architecture figure}
\begin{columns}
\column{.4\textwidth}
\begin{itemize}
\item Stack of N=6 layers
\item Position-wise feed forward network
\item Residual connection
\item Same dimesion output $d_{model}=512$
\end{itemize}

\column{.6\textwidth}
\begin{figure}[!h]
\includegraphics[width=\textwidth]{assets/model-fig.png}
\caption{fig}
\end{figure}

\end{columns}
\end{frame}


\begin{frame}{Attention architecture figure}
\begin{columns}

\column{.4\textwidth}
\begin{itemize}
\item SDPA: Attention(Q,K,V)=softmax($\frac{QK^{T}}{\sqrt{d_k}}$)V
\item MHA: MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^o, where $head_i=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$
\end{itemize}

\column{.6\textwidth}
\begin{figure}[!h]
\includegraphics[width=\textwidth]{assets/attention-fig.png}
\caption{fig}
\end{figure}

\end{columns}
\end{frame}

